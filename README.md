# Portuguese-Bank-Marketing
Machine Learning Analysis of a Portuguese Bank Marketing Campaign

Data source: https://archive.ics.uci.edu/dataset/222/bank+marketing

### Note
There are 6 datasets needed to run the process_notebook. There are 4 within the “raw” folder for the data exploration/preparation phase, and 2 within the “clean” folder that are needed for the machine learning portion of the analysis.

### Abstract
This project examined and analyzed data from a Portuguese bank marketing campaign related to term deposit accounts. The data is publicly available and was obtained from the UC Irvine Machine Learning Repository.

The primary goal of this project was to use machine learning for the classification task of predicting whether or not someone will sign up for this banking product. To the business – a hypothetical bank in this example – this would provide valuable information as well as actionable outcomes such as which groups to target for their products in order to maximize the number of people signing up.

As preparation for the classification task, the data went through various munging and preparation steps, such as ensuring the data was in a usable format (making sure all columns are formatted to the correct data type), sampling to ensure a balanced distribution of the target column, removing outliers, encoding categorical columns, and scaling the features to have 0 mean and standard deviation of 1.

In addition, to ensure that the models perform to the highest degree possible, techniques including Variance Inflation Factor and Chi-Square Contingency were used to assess any instances of multicollinearity, and principal component analysis was employed to mitigate any multicollinearity that was present. Finally, recursive feature elimination was employed to select only the columns that maximized accuracy using a simple Logistic Regression model. This ensured that the data was in a form that would allow for optimal performance in the machine learning portion of the project.

Once this initial data preparation was completed, the machine learning portion of the project could take place. This involved using 5 common classification algorithms – Random Forest, K-Nearest Neighbors, Decision Tree, Support Vector Machine, and Logistic Regression – to get initial results about the ability to predict outcomes on the dataset. For these baseline results, a Grid Search of possible hyperparameters was employed to ensure that the models were all adequately fit to the data.
  
Once these baseline results were in hand, a class was developed to further refine the hyperparameter optimization process for each algorithm. This was done by initializing the class with the particular model of interest, the hyperparameters as identified by the baseline results, and the relevant training and testing data. Then, the input hyperparameters were expanded in either direction, and three additional search algorithms were employed and compared to find the optimal combination of parameters for each algorithm. These search methods were Random Search, Bayes Search, and TPOT Genetic Algorithm. Employing these search methods produced mixed results, with the model that saw the greatest increase in performance – Support Vector Machine – gaining a 9 point increase in accuracy and a 17 point increase in Matthews Correlation Coefficient. Once each model was optimized, the ROC curve and AUC metric were plotted and computed for each algorithm. At the end of this entire process, the optimized Random Forest model performed the best in terms of accuracy, Matthews Correlation Coefficient, and AUC score. Not far behind were the optimized K-Nearest Neighbors and Support Vector machine models. Decision Tree and Logistic Regression proved less suited for this task in terms of the performance metrics that were computed.

As a final endeavor, an artificial neural network was employed with the goal of assigning a probability to each row. This was performed to a modest degree of accuracy, that was close to but not quite above 4 of the 5 algorithms discussed above. However, even with the accuracy of 83%, the neural network was able to successfully assign a probability to each row, and allowed some analysis of these probabilities. One very useful takeaway was identifying which rows were predicted with “high” confidence, which in this context was defined as being in either the top or bottom 10 percentile of probability values (being in the bottom 10 percentile means the model has high confidence in a “no” response).

This project has shown that it is possible to predict the outcomes of a campaign such as this using historical data, with the highest performing model – Random Forest – producing an accuracy of over 89%, and an AUC score of 0.95, which is a strong result.

Further, it has shown that it is possible to isolate those customers who are relatively very likely, or relatively very unlikely to sign up for a product such as this. By identifying the index of the rows that the neural network scored with the highest degree of confidence, it was possible to visualize which features had the greatest impact on the network’s decision making process, by identifying these rows as a subset of the original, unaltered dataset. Visualizations of the predictions made with the highest degree of confidence – both correct and incorrect – as well as only correct predictions – both yes and no responses – can be found at the end of the process notebook.
